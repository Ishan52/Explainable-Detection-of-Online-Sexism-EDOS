{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a70e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tarang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tarang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import contractions\n",
    "import demoji\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c98d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[lefti, sjw, simp, women, eat, basic, major, t...</td>\n",
       "      <td>4. prejudiced discussions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thank, lord, favor, whoso, findeth, wife, fin...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[anoth, democrat, signal, eyeglass, way, quali...</td>\n",
       "      <td>2. derogation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[met, girl, tinder, lost, felt, much, better, ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tri, wit, group, teenag, girl, none, could, p...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11195</th>\n",
       "      <td>[fuck, would, know, toxic, noth, pictur, first...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11196</th>\n",
       "      <td>[illeg, immigr, stronger, togeth, deport, mom,...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11197</th>\n",
       "      <td>[first, reaction, bulli, zoey, tuhr, shake, tr...</td>\n",
       "      <td>3. animosity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>[end, time, birth, pain, get, closer, togeth, ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11199</th>\n",
       "      <td>[lmao, boycott, american, women, mean, suppos,...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      [lefti, sjw, simp, women, eat, basic, major, t...   \n",
       "1      [thank, lord, favor, whoso, findeth, wife, fin...   \n",
       "2      [anoth, democrat, signal, eyeglass, way, quali...   \n",
       "3      [met, girl, tinder, lost, felt, much, better, ...   \n",
       "4      [tri, wit, group, teenag, girl, none, could, p...   \n",
       "...                                                  ...   \n",
       "11195  [fuck, would, know, toxic, noth, pictur, first...   \n",
       "11196  [illeg, immigr, stronger, togeth, deport, mom,...   \n",
       "11197  [first, reaction, bulli, zoey, tuhr, shake, tr...   \n",
       "11198  [end, time, birth, pain, get, closer, togeth, ...   \n",
       "11199  [lmao, boycott, american, women, mean, suppos,...   \n",
       "\n",
       "                  label_category  \n",
       "0      4. prejudiced discussions  \n",
       "1                           none  \n",
       "2                  2. derogation  \n",
       "3                           none  \n",
       "4                           none  \n",
       "...                          ...  \n",
       "11195                       none  \n",
       "11196                       none  \n",
       "11197               3. animosity  \n",
       "11198                       none  \n",
       "11199                       none  \n",
       "\n",
       "[11200 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('Training Data (TaskB).csv')\n",
    "\n",
    "for i in range(len(train_df.axes[0])):\n",
    "    words = train_df.iat[i,0][1:-1].split(',')\n",
    "    \n",
    "    text = []\n",
    "    for word in words:\n",
    "        word = word.strip()[1:-1]\n",
    "        text.append(word)\n",
    "        \n",
    "    train_df.iat[i,0] = text\n",
    "        \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7f62ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[damn, write, pretti, chaotic]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fuck, woman, still, mp]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[also, enough, evid, like, goldberg, anti, gg,...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fuck, nine, mm, get, arab, fuck, monkey]</td>\n",
       "      <td>1. threats, plans to harm and incitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[lefti, dad, offer, daughter, knock, maga, hat...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>[would, american, pleas, stop, upset, presid, ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>[mind, stay, home, mom, never, lift, finger]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>[know, come, revolut, much, guy, finish, child...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>[black, girl, pure, breed]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>[incel, girlfriend, fuck, anyon, say, kiss, le...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                        [damn, write, pretti, chaotic]   \n",
       "1                              [fuck, woman, still, mp]   \n",
       "2     [also, enough, evid, like, goldberg, anti, gg,...   \n",
       "3             [fuck, nine, mm, get, arab, fuck, monkey]   \n",
       "4     [lefti, dad, offer, daughter, knock, maga, hat...   \n",
       "...                                                 ...   \n",
       "2795  [would, american, pleas, stop, upset, presid, ...   \n",
       "2796       [mind, stay, home, mom, never, lift, finger]   \n",
       "2797  [know, come, revolut, much, guy, finish, child...   \n",
       "2798                         [black, girl, pure, breed]   \n",
       "2799  [incel, girlfriend, fuck, anyon, say, kiss, le...   \n",
       "\n",
       "                                label_category  \n",
       "0                                         none  \n",
       "1                                         none  \n",
       "2                                         none  \n",
       "3     1. threats, plans to harm and incitement  \n",
       "4                                         none  \n",
       "...                                        ...  \n",
       "2795                                      none  \n",
       "2796                                      none  \n",
       "2797                                      none  \n",
       "2798                                      none  \n",
       "2799                                      none  \n",
       "\n",
       "[2800 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('Testing Data (TaskB).csv')\n",
    "\n",
    "for i in range(len(test_df.axes[0])):\n",
    "    words = test_df.iat[i,0][1:-1].split(',')\n",
    "\n",
    "    text = []\n",
    "    for word in words:\n",
    "        word = word.strip()[1:-1]\n",
    "        text.append(word)\n",
    "\n",
    "    test_df.iat[i,0] = text\n",
    "        \n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9d2f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_category</th>\n",
       "      <th>label_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn, this writing was pretty chaotic</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, and apparently a bunch of misogynistic v...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How the FUCK is this woman still an MP!!!???</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understand. Know you're right. At same time I ...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surprized they didn't stop and rape some women</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>complexes like the 'nice chicks' that go after...</td>\n",
       "      <td>2. derogation</td>\n",
       "      <td>2.1 descriptive attacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>\"GRAPHIC Germany - Muslim \"refugee\" stabbing h...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>Lol I imagine there would be simps that are li...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>It's not, the girls I go on dates with don't k...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>How can he be an incel? He does have a girlfri...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label_category  \\\n",
       "0                  Damn, this writing was pretty chaotic           none   \n",
       "1      Yeah, and apparently a bunch of misogynistic v...           none   \n",
       "2           How the FUCK is this woman still an MP!!!???           none   \n",
       "3      Understand. Know you're right. At same time I ...           none   \n",
       "4         Surprized they didn't stop and rape some women           none   \n",
       "...                                                  ...            ...   \n",
       "13995  complexes like the 'nice chicks' that go after...  2. derogation   \n",
       "13996  \"GRAPHIC Germany - Muslim \"refugee\" stabbing h...           none   \n",
       "13997  Lol I imagine there would be simps that are li...           none   \n",
       "13998  It's not, the girls I go on dates with don't k...           none   \n",
       "13999  How can he be an incel? He does have a girlfri...           none   \n",
       "\n",
       "                  label_vector  \n",
       "0                         none  \n",
       "1                         none  \n",
       "2                         none  \n",
       "3                         none  \n",
       "4                         none  \n",
       "...                        ...  \n",
       "13995  2.1 descriptive attacks  \n",
       "13996                     none  \n",
       "13997                     none  \n",
       "13998                     none  \n",
       "13999                     none  \n",
       "\n",
       "[14000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Original Data.csv')\n",
    "df = df.drop(['rewire_id'], axis=1)\n",
    "df = df.drop(['label_sexist'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be86be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace('`', \"'\") #consistent quotes\n",
    "df['text'] = df['text'].str.replace('“', '\"') #consistent quotes\n",
    "df['text'] = df['text'].str.replace('”', '\"') #consistent quotes\n",
    "\n",
    "for i in range(len(df.axes[0])):\n",
    "    df.iat[i,0] = contractions.fix(df.iat[i,0]) #remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee8da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace('[0-9]*', \"\", regex=True) #remove digits\n",
    "df['text'] = df['text'].str.replace('\\\\[USER\\\\]|\\\\[URL\\\\]', \"\", regex=True) #remove \"URL\" and \"USER\"\n",
    "df['text'] = df['text'].str.lower() #convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfd5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(r'\\u200d', \"\", regex=True)\n",
    "df['text'] = df['text'].str.replace(r'\\u200f', \"\", regex=True)\n",
    "df['text'] = df['text'].str.replace(r'\\u200b', \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65bba54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_category</th>\n",
       "      <th>label_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[damn, write, pretti, chaotic]</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[yeah, appar, bunch, misogynist, virgin, one, ...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fuck, woman, still, mp]</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[understand, know, right, time, know, enough, ...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surpriz, stop, rape, women]</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>[complex, like, nice, chick, go, bad, boy, nah...</td>\n",
       "      <td>2. derogation</td>\n",
       "      <td>2.1 descriptive attacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>[graphic, germani, muslim, refuge, stab, young...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>[lol, imagin, would, simp, like, deserv, ugli,...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>[girl, go, date, kiss, first, date, text, back...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>[incel, girlfriend, fuck, anyon, say, kiss, le...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label_category  \\\n",
       "0                         [damn, write, pretti, chaotic]           none   \n",
       "1      [yeah, appar, bunch, misogynist, virgin, one, ...           none   \n",
       "2                               [fuck, woman, still, mp]           none   \n",
       "3      [understand, know, right, time, know, enough, ...           none   \n",
       "4                           [surpriz, stop, rape, women]           none   \n",
       "...                                                  ...            ...   \n",
       "13995  [complex, like, nice, chick, go, bad, boy, nah...  2. derogation   \n",
       "13996  [graphic, germani, muslim, refuge, stab, young...           none   \n",
       "13997  [lol, imagin, would, simp, like, deserv, ugli,...           none   \n",
       "13998  [girl, go, date, kiss, first, date, text, back...           none   \n",
       "13999  [incel, girlfriend, fuck, anyon, say, kiss, le...           none   \n",
       "\n",
       "                  label_vector  \n",
       "0                         none  \n",
       "1                         none  \n",
       "2                         none  \n",
       "3                         none  \n",
       "4                         none  \n",
       "...                        ...  \n",
       "13995  2.1 descriptive attacks  \n",
       "13996                     none  \n",
       "13997                     none  \n",
       "13998                     none  \n",
       "13999                     none  \n",
       "\n",
       "[14000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = string.punctuation + \"—\"\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for i in range(len(df.axes[0])):\n",
    "    text = df.iat[i,0]\n",
    "    text = text.translate(str.maketrans(punctuation, ' '*len(punctuation), '')) #remove punctuations\n",
    "    text = demoji.replace(text, \"\")\n",
    "    text = word_tokenize(text) #tokenize\n",
    "    text = [word for word in text if word not in stop_words] #remove stopwords\n",
    "    text = [porter.stem(word) for word in text] #stemming\n",
    "    df.iat[i,0] = text\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083b85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFrequencies(docs):\n",
    "    all_words = []\n",
    "\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            all_words.append(word)\n",
    "\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249056bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMostFrequentWords(k, all_words):\n",
    "    words = list(all_words.keys())[:k]\n",
    "    vocab = set()\n",
    "    for word in words:\n",
    "        vocab.add(word)\n",
    "\n",
    "    vocab = {word:idx for idx, word in enumerate(vocab)}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc719aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrueFalseFeatureSet(df, feature_words, size):\n",
    "    feature_vector = []\n",
    "\n",
    "    for i in range(len(df.axes[0])):\n",
    "        features = {}\n",
    "        text = df.iat[i,0]\n",
    "\n",
    "        for feature_word in feature_words:\n",
    "            features[feature_word] = feature_word in text\n",
    "\n",
    "        feature_vector.append(features)\n",
    "    \n",
    "    return [(feature_vector[i], df.iat[i,1]) for i in range(len(feature_vector))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983ad41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareCountFeatureSet(df, feature_words, size):\n",
    "    feature_vector = []\n",
    "\n",
    "    for i in range(len(df.axes[0])):\n",
    "        features = {}\n",
    "        text = df.iat[i,0]\n",
    "\n",
    "        for feature_word in feature_words:\n",
    "            features[feature_word] = 0\n",
    "            for word in text:\n",
    "                if word == feature_word:\n",
    "                    features[word] += 1\n",
    "\n",
    "        feature_vector.append(features)\n",
    "    \n",
    "    return [(feature_vector[i], df.iat[i,1]) for i in range(len(feature_vector))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc5bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnKNNClassifier(num_neighbours, train_featuresets, test_featuresets):\n",
    "    X_train = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in train_featuresets]\n",
    "    y_train = [feature_tag_tuple[1] for feature_tag_tuple in train_featuresets]\n",
    "    X_test = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in test_featuresets]\n",
    "    y_test = [feature_tag_tuple[1] for feature_tag_tuple in test_featuresets]\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=num_neighbours)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100, \"F1 Macro:\", metrics.f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee668bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnKNNClassifier(5, prepareTrueFalseFeatureSet(train_df, vocab, size), prepareTrueFalseFeatureSet(test_df, vocab, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c56bf4",
   "metadata": {},
   "source": [
    "#### scikit-learn k-NN Classifier (True/False Features Set)\n",
    "\n",
    "Number of neighbours = 3, Size of Feature Vector = 500, Accuracy = 71.86, F1 (Macro): 24.51<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 1000, Accuracy = 73.86, F1 (Macro): 27.98<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 1500, Accuracy = 73.43, F1 (Macro): 25.77<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 2000, Accuracy = 74.57, F1 (Macro): 25.67<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 3000, Accuracy = 76.18, F1 (Macro): 24.59<br><br>\n",
    "\n",
    "Number of neighbours = 7, Size of Feature Vector = 500, Accuracy = 75.61, F1 (Macro): 24.29<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 1000, Accuracy = 75.93, F1 (Macro): 24.41<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 1500, Accuracy = 75.68, F1 (Macro): 21.99<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 2000, Accuracy = 75.36, F1 (Macro): 22.53<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 3000, Accuracy = 76.07, F1 (Macro): 20.09<br><br>\n",
    "\n",
    "Number of neighbours = 15, Size of Feature Vector = 500, Accuracy = 75.71, F1 (Macro): 21.10<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 1000, Accuracy = 76.29, F1 (Macro): 19.93<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 1500, Accuracy = 76.29, F1 (Macro): 18.02<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 2000, Accuracy = 76.18, F1 (Macro): 18.09<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 3000, Accuracy = 76.21, F1 (Macro): 17.72<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed5fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnKNNClassifier(5, prepareCountFeatureSet(train_df, vocab, size), prepareCountFeatureSet(test_df, vocab, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c3bc6",
   "metadata": {},
   "source": [
    "#### scikit-learn k-NN Classifier (Count Features Set)\n",
    "\n",
    "Number of neighbours = 3, Size of Feature Vector = 500, Accuracy = 72.00, F1 (Macro): 24.21<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 1000, Accuracy = 73.11, F1 (Macro): 26.11<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 1500, Accuracy = 73.43, F1 (Macro): 25.67<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 2000, Accuracy = 74.07, F1 (Macro): 24.25<br>\n",
    "Number of neighbours = 3, Size of Feature Vector = 3000, Accuracy = 75.43, F1 (Macro): 23.91<br><br>\n",
    "\n",
    "Number of neighbours = 7, Size of Feature Vector = 500, Accuracy = 75.07, F1 (Macro): 24.15<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 1000, Accuracy = 75.89, F1 (Macro): 23.47<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 1500, Accuracy = 76.11, F1 (Macro): 22.11<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 2000, Accuracy = 76.18, F1 (Macro): 23.23<br>\n",
    "Number of neighbours = 7, Size of Feature Vector = 3000, Accuracy = 76.43, F1 (Macro): 21.31<br><br>\n",
    "\n",
    "Number of neighbours = 15, Size of Feature Vector = 500, Accuracy = 77.11, F1 (Macro): 55.59<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 1000, Accuracy = 78.75, F1 (Macro): 55.54<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 1500, Accuracy = 76.75, F1 (Macro): 48.43<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 2000, Accuracy = 77.18, F1 (Macro): 49.02<br>\n",
    "Number of neighbours = 15, Size of Feature Vector = 3000, Accuracy = 76.50, F1 (Macro): 46.29<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f508e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnLinearSVC(train_featuresets, test_featuresets):\n",
    "    X_train = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in train_featuresets]\n",
    "    y_train = [feature_tag_tuple[1] for feature_tag_tuple in train_featuresets]\n",
    "    X_test = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in test_featuresets]\n",
    "    y_test = [feature_tag_tuple[1] for feature_tag_tuple in test_featuresets]\n",
    "    \n",
    "    clf = LinearSVC(fit_intercept=False, loss='hinge', max_iter=5000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred), \"F1 Macro:\", metrics.f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc99db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnLinearSVC(prepareTrueFalseFeatureSet(train_df, vocab, 1000), prepareTrueFalseFeatureSet(test_df, vocab, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c03ee6",
   "metadata": {},
   "source": [
    "#### scikit-learn Linear SVC (True/False Features Set)\n",
    "\n",
    "With feature vector of size 500, Accuracy: 72.29, F1 (Macro): 25.52<br>\n",
    "With feature vector of size 1000, Accuracy: 73.64, F1 (Macro): 28.90<br>\n",
    "With feature vector of size 1500, Accuracy: 72.86, F1 (Macro): 30.18<br>\n",
    "With feature vector of size 2000, Accuracy: 72.61, F1 (Macro): 30.95<br>\n",
    "With feature vector of size 2500, Accuracy: 72.18, F1 (Macro): 31.95<br>\n",
    "With feature vector of size 3000, Accuracy: 72.32, F1 (Macro): 32.22<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4383b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnLinearSVC(prepareCountFeatureSet(train_df, vocab, 1000), prepareCountFeatureSet(test_df, vocab, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e97ce",
   "metadata": {},
   "source": [
    "#### scikit-learn Linear SVC (Count Features Set)\n",
    "\n",
    "With feature vector of size 500, Accuracy: 73.93, F1 (Macro): 25.69<br>\n",
    "With feature vector of size 1000, Accuracy: 73.96, F1 (Macro): 28.86<br>\n",
    "With feature vector of size 1500, Accuracy: 72.93, F1 (Macro): 29.47<br>\n",
    "With feature vector of size 2000, Accuracy: 72.86, F1 (Macro): 30.13<br>\n",
    "With feature vector of size 2500, Accuracy: 72.61, F1 (Macro): 31.48<br>\n",
    "With feature vector of size 3000, Accuracy: 72.00, F1 (Macro): 31.90<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c03a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnDecisionTree(train_featuresets, test_featuresets):\n",
    "    X_train = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in train_featuresets]\n",
    "    y_train = [feature_tag_tuple[1] for feature_tag_tuple in train_featuresets]\n",
    "    X_test = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in test_featuresets]\n",
    "    y_test = [feature_tag_tuple[1] for feature_tag_tuple in test_featuresets]\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=7)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred), \"F1 Macro:\", metrics.f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b79110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnDecisionTree(prepareTrueFalseFeatureSet(train_df, vocab, 1000), prepareTrueFalseFeatureSet(test_df, vocab, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa4fcb",
   "metadata": {},
   "source": [
    "#### scikit-learn Decision Tree Classifier (True/False Features Set)\n",
    "\n",
    "##### max_depth = 7\n",
    "\n",
    "With feature vector of size  500, Accuracy: 78.07, F1 (Macro): 25.59<br>\n",
    "With feature vector of size 1000, Accuracy: 79.39, F1 (Macro): 29.17<br>\n",
    "With feature vector of size 1500, Accuracy: 79.43, F1 (Macro): 29.24<br>\n",
    "With feature vector of size 2000, Accuracy: 79.43, F1 (Macro): 29.26<br>\n",
    "With feature vector of size 3000, Accuracy: 79.57, F1 (Macro): 29.60<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6e9fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnDecisionTree(prepareCountFeatureSet(train_df, vocab, 1000), prepareCountFeatureSet(test_df, vocab, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c2431",
   "metadata": {},
   "source": [
    "#### scikit-learn Decision Tree Classifier (Count Features Set)\n",
    "\n",
    "##### max_depth = 7\n",
    "\n",
    "With feature vector of size  500, Accuracy: 78.25, F1 (Macro): 26.52<br>\n",
    "With feature vector of size 1000, Accuracy: 79.43, F1 (Macro): 29.29<br>\n",
    "With feature vector of size 1500, Accuracy: 79.50, F1 (Macro): 29.43<br>\n",
    "With feature vector of size 2000, Accuracy: 79.50, F1 (Macro): 29.43<br>\n",
    "With feature vector of size 3000, Accuracy: 79.64, F1 (Macro): 29.77<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "579668b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnRandomForest(train_featuresets, test_featuresets):\n",
    "    X_train = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in train_featuresets]\n",
    "    y_train = [feature_tag_tuple[1] for feature_tag_tuple in train_featuresets]\n",
    "    X_test = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in test_featuresets]\n",
    "    y_test = [feature_tag_tuple[1] for feature_tag_tuple in test_featuresets]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred), \"F1 Macro:\", metrics.f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2dad15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "# sklearnRandomForest(prepareTrueFalseFeatureSet(train_df, vocab, 1000), prepareTrueFalseFeatureSet(test_df, vocab, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f00245",
   "metadata": {},
   "source": [
    "#### scikit-learn Random Forest Classifier (True/False Features Set)\n",
    "\n",
    "##### num_estimators = 100\n",
    "\n",
    "With feature vector of size 500, Accuracy: 76.32, F1 (Macro): 28.56<br>\n",
    "With feature vector of size 1000, Accuracy: 78.29, F1 (Macro): 30.84<br>\n",
    "With feature vector of size 1500, Accuracy: 78.64, F1 (Macro): 31.50<br>\n",
    "With feature vector of size 2000, Accuracy: 78.57, F1 (Macro): 32.86<br>\n",
    "With feature vector of size 3000, Accuracy: 79.11, F1 (Macro): 31.90<br>\n",
    "With feature vector of size 4000, Accuracy: 79.46, F1 (Macro): 32.53<br>\n",
    "With feature vector of size 5000, Accuracy: 79.61, F1 (Macro): 32.06<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3b9d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = kMostFrequentWords(1000, wordFrequencies(train_df['text']))\n",
    "# sklearnRandomForest(prepareCountFeatureSet(train_df, vocab, 1000), prepareCountFeatureSet(test_df, vocab, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df0be7",
   "metadata": {},
   "source": [
    "#### scikit-learn Random Forest Classifier (Count Features Set)\n",
    "\n",
    "##### num_estimators = 100\n",
    "\n",
    "With feature vector of size 500, Accuracy: 76.46, F1 (Macro): 28.83<br>\n",
    "With feature vector of size 1000, Accuracy: 78.61, F1 (Macro): 31.48<br>\n",
    "With feature vector of size 1500, Accuracy: 78.68, F1 (Macro): 32.32<br>\n",
    "With feature vector of size 2000, Accuracy: 79.07, F1 (Macro): 33.28<br>\n",
    "With feature vector of size 3000, Accuracy: 79.29, F1 (Macro): 32.63<br>\n",
    "With feature vector of size 4000, Accuracy: 79.64, F1 (Macro): 31.93<br>\n",
    "With feature vector of size 5000, Accuracy: 79.61, F1 (Macro): 32.17<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72504c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnMultinomialNB(train_featuresets, test_featuresets):\n",
    "    X_train = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in train_featuresets]\n",
    "    y_train = [feature_tag_tuple[1] for feature_tag_tuple in train_featuresets]\n",
    "    X_test = [list(feature_tag_tuple[0].values()) for feature_tag_tuple in test_featuresets]\n",
    "    y_test = [feature_tag_tuple[1] for feature_tag_tuple in test_featuresets]\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval = int(len(df)*0.2)\n",
    "\n",
    "# for size in [500, 1000, 1500, 2000, 2500, 3000]:\n",
    "#     print(\"\\nSize: \", size)\n",
    "    \n",
    "#     acc_sum = 0\n",
    "#     f1_sum = 0\n",
    "    \n",
    "#     for i in range(5):\n",
    "#         train_df = pd.concat([df[0:i*interval], df[(i+1)*interval:len(df)]], axis=0)\n",
    "#         test_df = df[i*interval:(i+1)*interval]\n",
    "    \n",
    "#         vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "#         acc, f1 = sklearnMultinomialNB(prepareTrueFalseFeatureSet(train_df, vocab, size), prepareTrueFalseFeatureSet(test_df, vocab, size))\n",
    "#         print(\"Accuracy: \", acc, \", F1 Score: \", f1)\n",
    "#         acc_sum += acc\n",
    "#         f1_sum += f1\n",
    "        \n",
    "#     print(\"Average Accuracy: \", acc_sum/5, \", Average F1 Score: \", f1_sum/5)\n",
    "\n",
    "# sklearnMultinomialNB(prepareTrueFalseFeatureSet(train_df, 1000), prepareTrueFalseFeatureSet(test_df, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c72fc7",
   "metadata": {},
   "source": [
    "#### scikit-learn Multinomial Naive Bayes (True/False Features Set)\n",
    "\n",
    "With feature vector of size  500, Accuracy: 77.88, F1 (Macro): 29.62<br>\n",
    "With feature vector of size 1000, Accuracy: 77.80, F1 (Macro): 31.35<br>\n",
    "With feature vector of size 1500, Accuracy: 77.72, F1 (Macro): 31.98<br>\n",
    "With feature vector of size 2000, Accuracy: 77.65, F1 (Macro): 32.54<br>\n",
    "With feature vector of size 2500, Accuracy: 77.64, F1 (Macro): 31.67<br>\n",
    "With feature vector of size 3000, Accuracy: 77.56, F1 (Macro): 31.31<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c59e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval = int(len(df)*0.2)\n",
    "\n",
    "# for size in [500, 1000, 1500, 2000, 2500, 3000]:\n",
    "#     print(\"\\nSize: \", size)\n",
    "    \n",
    "#     acc_sum = 0\n",
    "#     f1_sum = 0\n",
    "    \n",
    "#     for i in range(5):\n",
    "#         train_df = pd.concat([df[0:i*interval], df[(i+1)*interval:len(df)]], axis=0)\n",
    "#         test_df = df[i*interval:(i+1)*interval]\n",
    "    \n",
    "#         vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "#         acc, f1 = sklearnMultinomialNB(prepareCountFeatureSet(train_df, vocab, size), prepareCountFeatureSet(test_df, vocab, size))\n",
    "#         print(\"Accuracy: \", acc, \", F1 Score: \", f1)\n",
    "#         acc_sum += acc\n",
    "#         f1_sum += f1\n",
    "        \n",
    "#     print(\"Average Accuracy: \", acc_sum/5, \", Average F1 Score: \", f1_sum/5)\n",
    "    \n",
    "# sklearnMultinomialNB(prepareCountFeatureSet(train_df, 1000), prepareCountFeatureSet(test_df, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994703cf",
   "metadata": {},
   "source": [
    "#### scikit-learn Multinomial Naive Bayes (Count Features Set)\n",
    "\n",
    "With feature vector of size  500, Accuracy: 77.45, F1 (Macro): 30.16<br>\n",
    "With feature vector of size 1000, Accuracy: 77.37, F1 (Macro): 32.45<br>\n",
    "With feature vector of size 1500, Accuracy: 77.15, F1 (Macro): 33.01<br>\n",
    "With feature vector of size 2000, Accuracy: 77.15, F1 (Macro): 32.96<br>\n",
    "With feature vector of size 2500, Accuracy: 76.99, F1 (Macro): 32.54<br>\n",
    "With feature vector of size 3000, Accuracy: 77.05, F1 (Macro): 31.89<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7da438ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltkMultinomialNB(train_featuresets, test_featuresets):\n",
    "    clf = nltk.NaiveBayesClassifier.train(train_featuresets)\n",
    "    \n",
    "    preds = []\n",
    "    tests = [test_featuresets[i][0] for i in range(len(test_featuresets))]\n",
    "    for test in tests:\n",
    "        preds.append(clf.classify(test))\n",
    "    \n",
    "    y_test = [feature_tag_tuple[1] for feature_tag_tuple in test_featuresets]\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == y_test[i]:\n",
    "            cnt += 1\n",
    "        \n",
    "    return cnt/len(preds), metrics.f1_score(y_test, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c604fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval = int(len(df)*0.2)\n",
    "\n",
    "# for size in [500, 1000, 1500, 2000, 2500, 3000]:\n",
    "#     print(\"\\nSize: \", size)\n",
    "    \n",
    "#     acc_sum = 0\n",
    "#     f1_sum = 0\n",
    "    \n",
    "#     for i in range(5):\n",
    "#         train_df = pd.concat([df[0:i*interval], df[(i+1)*interval:len(df)]], axis=0)\n",
    "#         test_df = df[i*interval:(i+1)*interval]\n",
    "    \n",
    "#         vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "#         acc, f1 = nltkMultinomialNB(prepareTrueFalseFeatureSet(train_df, vocab, size), prepareTrueFalseFeatureSet(test_df, vocab, size))\n",
    "#         print(\"Accuracy: \", acc, \", F1 Score: \", f1)\n",
    "#         acc_sum += acc\n",
    "#         f1_sum += f1\n",
    "        \n",
    "#     print(\"Average Accuracy: \", acc_sum/5, \", Average F1 Score: \", f1_sum/5)\n",
    "    \n",
    "# nltkMultinomialNB(prepareTrueFalseFeatureSet(train_df, 1000), prepareTrueFalseFeatureSet(test_df, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090646eb",
   "metadata": {},
   "source": [
    "#### NLTK Multinomial Naive Bayes (True/False Features Set)\n",
    "\n",
    "With feature vector of size  500, Accuracy: 77.06, F1 (Macro): 31.19<br>\n",
    "With feature vector of size 1000, Accuracy: 77.34, F1 (Macro): 34.74<br>\n",
    "With feature vector of size 1500, Accuracy: 76.74, F1 (Macro): 34.82<br>\n",
    "With feature vector of size 2000, Accuracy: 76.62, F1 (Macro): 34.30<br>\n",
    "With feature vector of size 2500, Accuracy: 76.54, F1 (Macro): 33.76<br>\n",
    "With feature vector of size 3000, Accuracy: 76.55, F1 (Macro): 33.50<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6cd7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval = int(len(df)*0.2)\n",
    "\n",
    "# for size in [500, 1000, 1500, 2000, 2500, 3000]:\n",
    "#     print(\"\\nSize: \", size)\n",
    "    \n",
    "#     acc_sum = 0\n",
    "#     f1_sum = 0\n",
    "    \n",
    "#     for i in range(5):\n",
    "#         train_df = pd.concat([df[0:i*interval], df[(i+1)*interval:len(df)]], axis=0)\n",
    "#         test_df = df[i*interval:(i+1)*interval]\n",
    "    \n",
    "#         vocab = kMostFrequentWords(size, wordFrequencies(train_df['text']))\n",
    "#         acc, f1 = nltkMultinomialNB(prepareCountFeatureSet(train_df, vocab, size), prepareCountFeatureSet(test_df, vocab, size))\n",
    "#         print(\"Accuracy: \", acc, \", F1 Score: \", f1)\n",
    "#         acc_sum += acc\n",
    "#         f1_sum += f1\n",
    "        \n",
    "#     print(\"Average Accuracy: \", acc_sum/5, \", Average F1 Score: \", f1_sum/5)\n",
    "\n",
    "# nltkMultinomialNB(prepareCountFeatureSet(train_df, 1000), prepareCountFeatureSet(test_df, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079315b",
   "metadata": {},
   "source": [
    "#### NLTK Multinomial Naive Bayes (Count Features Set)\n",
    "\n",
    "With feature vector of size  500, Accuracy: 76.49, F1 (Macro): 30.42<br>\n",
    "With feature vector of size 1000, Accuracy: 76.32, F1 (Macro): 32.00<br>\n",
    "With feature vector of size 1500, Accuracy: 75.79, F1 (Macro): 30.88<br>\n",
    "With feature vector of size 2000, Accuracy: 75.82, F1 (Macro): 31.00<br>\n",
    "With feature vector of size 2500, Accuracy: 75.84, F1 (Macro): 30.35<br>\n",
    "With feature vector of size 3000, Accuracy: 75.65, F1 (Macro): 29.48<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218e318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
